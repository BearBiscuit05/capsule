{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as MF\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl.data import AsNodePredDataset\n",
    "from dgl.dataloading import DataLoader, NeighborSampler, MultiLayerFullNeighborSampler\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "import os\n",
    "import numpy as np\n",
    "from dgl.dataloading import (\n",
    "    as_edge_prediction_sampler,\n",
    "    DataLoader,\n",
    "    MultiLayerFullNeighborSampler,\n",
    "    negative_sampler,\n",
    "    NeighborSampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AsNodePredDataset(DglNodePropPredDataset('ogbn-products',root=\"/home/bear/workspace/singleGNN/data/dataset\"))\n",
    "folder_path=\"/home/bear/workspace/singleGNN/data/dataset/ogbn_products/split_lp\"\n",
    "os.makedirs(folder_path)\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Products数据集\n",
    "nodeNUM:2449029\n",
    "edgeNUM:123718152\n",
    "feat:100\n",
    "class:47\n",
    "trainNUM:196615\n",
    "valNUM:39323\n",
    "testNUM:2213091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "edgeNUM = 1000000\n",
    "neg_num = 1000\n",
    "sampled_edge_ids = random.sample(range(g.num_edges()), edgeNUM)\n",
    "trainId=sampled_edge_ids[:int(edgeNUM * 0.3)]\n",
    "TestId=sampled_edge_ids[int(edgeNUM * 0.3):int(edgeNUM * 0.9)]\n",
    "ValId=sampled_edge_ids[int(edgeNUM * 0.9):]\n",
    "\n",
    "\n",
    "neg_train_sampler = dgl.dataloading.negative_sampler.Uniform(1)\n",
    "train_src, train_neg_dst = neg_train_sampler(g, torch.Tensor(trainId).to(torch.int64))\n",
    "raw_train_src=torch.Tensor(g.edges()[0][trainId])\n",
    "raw_train_dst=torch.Tensor(g.edges()[1][trainId])\n",
    "\n",
    "\n",
    "neg_val_sampler = dgl.dataloading.negative_sampler.Uniform(neg_num)\n",
    "val_src,val_neg_dst = neg_val_sampler(g, torch.Tensor(ValId).to(torch.int64))\n",
    "raw_val_src=torch.Tensor(g.edges()[0][ValId])\n",
    "raw_val_dst=torch.Tensor(g.edges()[1][ValId])\n",
    "val_neg_dst.reshape(-1,neg_num)\n",
    "\n",
    "neg_test_sampler = dgl.dataloading.negative_sampler.Uniform(neg_num)\n",
    "test_src, test_neg_dst = neg_test_sampler(g, torch.Tensor(TestId).to(torch.int64))\n",
    "raw_test_src=torch.Tensor(g.edges()[0][TestId])\n",
    "raw_test_dst=torch.Tensor(g.edges()[1][TestId])\n",
    "test_neg_dst.reshape(-1,neg_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'train': {'source_node': tensor([ 837791,  837791,  837791,  ...,  928474, 1521923, 1257024]),\n",
    "  'target_node': tensor([    102, 1074631, 1658157,  ...,  250277,  316321,  325910])},\n",
    " 'valid': {'source_node': tensor([2302382, 1112061, 1660093,  ..., 1660079, 2395010,   88681]),\n",
    "  'target_node': tensor([ 245742,  839129,  228003,  ...,  699000, 2485204,  849448]),\n",
    "  'target_node_neg': tensor([[2415348,  412967, 2565382,  ..., 1353044, 1652394, 1627182],\n",
    "          ...,\n",
    "          [ 576938, 2577235, 2070601,  ...,  155330, 1643711, 1166252]])},\n",
    " 'test': {'source_node': tensor([2302382, 1112061, 1660093,  ..., 1660079, 2395010,   88681]),\n",
    "  'target_node': tensor([1611317,  963234,  108714,  ..., 2537726,  163858,  112969]),\n",
    "  'target_node_neg': tensor([[ 114076, 2093394, 1308822,  ..., 2321062, 1852878,  936672],\n",
    "          ...,\n",
    "          [2587770, 2587954,   49452,  ..., 1938683, 1319512, 2029363]])}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lp={}\n",
    "split_lp['train']={}\n",
    "split_lp['train']['source_node']=raw_train_src\n",
    "split_lp['train']['target_node']=raw_train_dst\n",
    "split_lp['train']['target_node_neg']=raw_train_src\n",
    "\n",
    "split_lp['valid']={}\n",
    "split_lp['valid']['source_node']=raw_val_src\n",
    "split_lp['valid']['target_node']=raw_val_dst\n",
    "split_lp['valid']['target_node_neg']=val_neg_dst.reshape(-1,neg_num)\n",
    "\n",
    "split_lp['test']={}\n",
    "split_lp['test']['source_node']=raw_test_src\n",
    "split_lp['test']['target_node']=raw_test_dst\n",
    "split_lp['test']['target_node_neg']=test_neg_dst.reshape(-1,neg_num)\n",
    "\n",
    "split_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = folder_path+\"/split_dict.pt\"\n",
    "torch.save(split_lp, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curDir = \"/home/bear/workspace/singleGNN/src/predata/LP\"\n",
    "def load_reddit(self_loop=True):\n",
    "    from dgl.data import RedditDataset\n",
    "    data = RedditDataset(self_loop=self_loop,raw_dir=curDir+'/../../../data/dataset/')\n",
    "    g = data[0]\n",
    "    g.ndata['feat'] = g.ndata.pop('feat')\n",
    "    g.ndata['label'] = g.ndata.pop('label')\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    for index in range(len(g.ndata['train_mask'])):\n",
    "        if g.ndata['train_mask'][index] == 1:\n",
    "            train_idx.append(index)\n",
    "    for index in range(len(g.ndata['val_mask'])):\n",
    "        if g.ndata['val_mask'][index] == 1:\n",
    "            val_idx.append(index)\n",
    "    for index in range(len(g.ndata['test_mask'])):\n",
    "        if g.ndata['test_mask'][index] == 1:\n",
    "            test_idx.append(index)\n",
    "    return g, data,train_idx,val_idx,test_idx\n",
    "\n",
    "g, Testdata , train_idx , val_idx,test_idx = load_reddit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reddit数据集\n",
    "nodeNUM:232965\n",
    "edgeNUM:114848857\n",
    "feat:602\n",
    "class:41\n",
    "trainNUM:153431\n",
    "valNUM:23831\n",
    "testNUM:55703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "edgeNUM = 1000000\n",
    "neg_num = 1000\n",
    "sampled_edge_ids = random.sample(range(g.num_edges()), edgeNUM)\n",
    "trainId=sampled_edge_ids[:int(edgeNUM * 0.3)]\n",
    "TestId=sampled_edge_ids[int(edgeNUM * 0.3):int(edgeNUM * 0.9)]\n",
    "ValId=sampled_edge_ids[int(edgeNUM * 0.9):]\n",
    "\n",
    "\n",
    "neg_train_sampler = dgl.dataloading.negative_sampler.Uniform(1)\n",
    "train_src, train_neg_dst = neg_train_sampler(g, torch.Tensor(trainId).to(torch.int64))\n",
    "raw_train_src=torch.Tensor(g.edges()[0][trainId])\n",
    "raw_train_dst=torch.Tensor(g.edges()[1][trainId])\n",
    "\n",
    "\n",
    "neg_val_sampler = dgl.dataloading.negative_sampler.Uniform(neg_num)\n",
    "val_src,val_neg_dst = neg_val_sampler(g, torch.Tensor(ValId).to(torch.int64))\n",
    "raw_val_src=torch.Tensor(g.edges()[0][ValId])\n",
    "raw_val_dst=torch.Tensor(g.edges()[1][ValId])\n",
    "val_neg_dst.reshape(-1,neg_num)\n",
    "\n",
    "neg_test_sampler = dgl.dataloading.negative_sampler.Uniform(neg_num)\n",
    "test_src, test_neg_dst = neg_test_sampler(g, torch.Tensor(TestId).to(torch.int64))\n",
    "raw_test_src=torch.Tensor(g.edges()[0][TestId])\n",
    "raw_test_dst=torch.Tensor(g.edges()[1][TestId])\n",
    "test_neg_dst.reshape(-1,neg_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lp={}\n",
    "split_lp['train']={}\n",
    "split_lp['train']['source_node']=raw_train_src\n",
    "split_lp['train']['target_node']=raw_train_dst\n",
    "split_lp['train']['target_node_neg']=raw_train_src\n",
    "\n",
    "split_lp['valid']={}\n",
    "split_lp['valid']['source_node']=raw_val_src\n",
    "split_lp['valid']['target_node']=raw_val_dst\n",
    "split_lp['valid']['target_node_neg']=val_neg_dst.reshape(-1,neg_num)\n",
    "\n",
    "split_lp['test']={}\n",
    "split_lp['test']['source_node']=raw_test_src\n",
    "split_lp['test']['target_node']=raw_test_dst\n",
    "split_lp['test']['target_node_neg']=test_neg_dst.reshape(-1,neg_num)\n",
    "\n",
    "split_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=curDir+'/../../../data/dataset/split_lp'\n",
    "os.makedirs(folder_path)\n",
    "file_name = folder_path+\"/split_dict.pt\"\n",
    "torch.save(split_lp, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
