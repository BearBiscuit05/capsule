{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as MF\n",
    "import dgl.nn as dglnn\n",
    "from dgl.data import AsNodePredDataset\n",
    "from dgl.dataloading import DataLoader, NeighborSampler, MultiLayerFullNeighborSampler\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "import tqdm\n",
    "import argparse\n",
    "import ast\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_size, hid_size, out_size,num_layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # three-layer GraphSAGE-mean\n",
    "        self.layers.append(dglnn.SAGEConv(in_size, hid_size, 'mean'))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(dglnn.SAGEConv(hid_size, hid_size, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(hid_size, out_size, 'mean'))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hid_size = hid_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            h = layer(block, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = F.relu(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g,device, batch_size):\n",
    "        \"\"\"Conduct layer-wise inference to get all the node embeddings.\"\"\"\n",
    "        feat = g.ndata['feat']\n",
    "        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])\n",
    "        # sampler = NeighborSampler([15],  # fanout for [layer-0, layer-1, layer-2]\n",
    "        #                     prefetch_node_feats=['feat'],\n",
    "        #                     prefetch_labels=['label'])\n",
    "        dataloader = DataLoader(\n",
    "                g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,\n",
    "                batch_size=batch_size, shuffle=False, drop_last=False,\n",
    "                num_workers=0)\n",
    "        buffer_device = torch.device('cpu')\n",
    "        pin_memory = (buffer_device != device)\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = torch.empty(\n",
    "                g.num_nodes(), self.hid_size if l != len(self.layers) - 1 else self.out_size,\n",
    "                device=buffer_device, pin_memory=pin_memory)\n",
    "            feat = feat.to(device)\n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                x = feat[input_nodes]\n",
    "                h = layer(blocks[0], x) # len(blocks) = 1\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = F.relu(h)\n",
    "                    h = self.dropout(h)\n",
    "                # by design, our output nodes are contiguous\n",
    "                y[output_nodes[0]:output_nodes[-1]+1] = h.to(buffer_device)\n",
    "            feat = y\n",
    "        return y\n",
    "\n",
    "def evaluate(model, graph, dataloader):\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    y_hats = []\n",
    "    for it, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            x = blocks[0].srcdata['feat']\n",
    "            ys.append(blocks[-1].dstdata['label'].cpu().numpy())\n",
    "            y_hats.append(model(blocks, x).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(y_hats)\n",
    "        labels = np.concatenate(ys)\n",
    "    return sklearn.metrics.accuracy_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AsNodePredDataset(DglNodePropPredDataset('ogbn-papers100M',root=\"/home/bear/workspace/singleGNN/data/dataset\"))\n",
    "g = dataset[0]\n",
    "\n",
    "# dataset = AsNodePredDataset(DglNodePropPredDataset('ogbn-products',root=\"/home/bear/workspace/singleGNN/data/dataset\"))\n",
    "# g = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=111059956, num_edges=1615685872,\n",
       "      ndata_schemes={'test_mask': Scheme(shape=(), dtype=torch.uint8), 'val_mask': Scheme(shape=(), dtype=torch.uint8), 'train_mask': Scheme(shape=(), dtype=torch.uint8), 'feat': Scheme(shape=(128,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.float32), 'year': Scheme(shape=(1,), dtype=torch.int64)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src = g.edges()[0].numpy()\n",
    "dst = g.edges()[1].numpy()\n",
    "src.tofile(\"/raid/bear/products_bin/srcList.bin\")\n",
    "dst.tofile(\"/raid/bear/products_bin/dstList.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = g.ndata['feat'].numpy().tofile(\"/raid/bear/products_bin/feat.bin\")\n",
    "label = g.ndata['label'].numpy().tofile(\"/raid/bear/products_bin/label.bin\")\n",
    "# feat.shape\n",
    "# label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nonzero(g.ndata['train_mask']).squeeze().numpy().tofile(\"/raid/bear/products_bin/trainIDs.bin\")\n",
    "torch.nonzero(g.ndata['val_mask']).squeeze().numpy().tofile(\"/raid/bear/products_bin/valIDs.bin\")\n",
    "torch.nonzero(g.ndata['test_mask']).squeeze().numpy().tofile(\"/raid/bear/products_bin/testIDs.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "in_size = g.ndata['feat'].shape[1]\n",
    "out_size = dataset.num_classes\n",
    "model = SAGE(in_size, 256, out_size,2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = NeighborSampler([10, 10],\n",
    "                              prefetch_node_feats=['feat'],\n",
    "                              prefetch_labels=['label'])\n",
    "train_idx = dataset.train_idx\n",
    "val_idx = dataset.val_idx\n",
    "test_idx = dataset.test_idx\n",
    "device = torch.device('cpu')\n",
    "train_dataloader = DataLoader(g, train_idx, sampler, device=device,\n",
    "                                  batch_size=1024, shuffle=True,\n",
    "                                  drop_last=False, num_workers=8)\n",
    "val_dataloader = DataLoader(g, val_idx, sampler, device=device,\n",
    "                                batch_size=1024, shuffle=True,\n",
    "                                drop_last=False, num_workers=8,\n",
    "                                )\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = NeighborSampler([15, 50],\n",
    "                              prefetch_node_feats=['feat'],\n",
    "                              prefetch_labels=['label'])\n",
    "train_idx = dataset.train_idx\n",
    "val_idx = dataset.val_idx\n",
    "test_idx = dataset.test_idx\n",
    "device = torch.device('cuda:0')\n",
    "train_dataloader = DataLoader(g, train_idx, sampler, device=device,\n",
    "                                  batch_size=1024, shuffle=True,\n",
    "                                  drop_last=False, num_workers=0,use_uva=True)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "t_list= []\n",
    "out_list = []\n",
    "edges = [[],[]]\n",
    "\n",
    "for epoch in range(1):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    startTime = time.time()\n",
    "    count = 0\n",
    "    for it, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):\n",
    "        t_list.extend(input_nodes)\n",
    "        out_list.extend(output_nodes)\n",
    "        edges[0].extend(input_nodes[blocks[0].edges()[0]])\n",
    "        edges[0].extend(input_nodes[blocks[1].edges()[0]])\n",
    "        edges[1].extend(input_nodes[blocks[0].edges()[1]])\n",
    "        edges[1].extend(input_nodes[blocks[1].edges()[1]])\n",
    "unique_input_elements = t_list.unique()\n",
    "unique_seed_elements = out_list.unique()\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subG_feats = g.ndata['feat'][unique_input_elements]\n",
    "subG_labels = g.ndata['label'][unique_seed_elements]\n",
    "src = torch.cat(edges[0])\n",
    "dst = torch.cat(edges[1])\n",
    "test_g = dgl.graph((src, dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(unique_elements.shape)\n",
    "subG_feats = g.ndata['feat'][unique_input_elements]\n",
    "print(subG_feats.shape)\n",
    "subG_labels = g.ndata['label'][unique_seed_elements]\n",
    "src = torch.cat(edges[0])\n",
    "dst = torch.cat(edges[1])\n",
    "print(subG_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_g = dgl.graph((src, dst))\n",
    "test_g.ndata['feat'] = g.ndata['feat'][:111059954]\n",
    "test_g.ndata['label'] = g.ndata['label'][:111059954].to(torch.int64)\n",
    "print(test_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = unique_seed_elements\n",
    "seed_sampler = NeighborSampler([5, 5],\n",
    "                              prefetch_node_feats=['feat'],\n",
    "                              prefetch_labels=['label'])\n",
    "trains_dataloader = DataLoader(test_g, seeds, seed_sampler, device=device,\n",
    "                                  batch_size=1024, shuffle=True,\n",
    "                                  drop_last=False, num_workers=8)\n",
    "\n",
    "with trains_dataloader.enable_cpu_affinity():\n",
    "    for epoch in range(10):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        startTime = time.time()\n",
    "        count = 0\n",
    "        for it, (input_nodes, output_nodes, blocks) in enumerate(trains_dataloader):\n",
    "            x = blocks[0].srcdata['feat']\n",
    "            y = blocks[-1].dstdata['label']\n",
    "            y_hat = model(blocks, x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "            count = it\n",
    "        print(\"count=\",count)\n",
    "        print(\"time=\",time.time()-startTime)\n",
    "        acc = evaluate(model, g, val_dataloader)\n",
    "        print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} \"\n",
    "                .format(epoch, total_loss / (it+1), acc.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
