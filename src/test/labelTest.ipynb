{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bear/miniconda3/envs/dgl_v09/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First each training point has its own label (idx)\n",
    "# The tag is then passed dst <-> src\n",
    "# After n rounds, each point has its own final label\n",
    "# The clustering results are then merged\n",
    "\n",
    "graph = torch.as_tensor(np.fromfile(\"/raid/bear/data/raw/papers100M/graph1.bin\",dtype=np.int32))\n",
    "trainids = torch.as_tensor(np.fromfile(\"/raid/bear/data/raw/papers100M/trainIds.bin\",dtype=np.int64))\n",
    "src = graph[::2]\n",
    "dst = graph[1::2]\n",
    "src = src.cuda()\n",
    "dst = dst.cuda()\n",
    "\n",
    "nodeNUM = 111059956\n",
    "nodeLabel = torch.zeros(nodeNUM).to(torch.int32) -1\n",
    "nodeLabel[trainids] = trainids.to(torch.int32)\n",
    "nodeLabel = nodeLabel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl.lpGraph(src,dst,nodeLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear storage\n",
    "src = src.cpu()\n",
    "dst = dst.cpu()\n",
    "nodeLabel = nodeLabel.cpu()\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the training set label\n",
    "trainLabel = nodeLabel[trainids.to(torch.int64)]\n",
    "\n",
    "# Clustering by label (quantity varies)\n",
    "labelCluster = torch.bincount(trainLabel)\n",
    "sortLabelsNUM,labelIdx = torch.sort(labelCluster,descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = len(torch.nonzero(sortLabelsNUM > 0).reshape(-1))\n",
    "sortLabelsNUM = sortLabelsNUM[:bound]\n",
    "labelIdx = labelIdx[:bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMinPart(partInfo):\n",
    "    min_value, min_index = torch.min(partInfo, dim=0)\n",
    "    return min_value, min_index\n",
    "\n",
    "def findMaxPart(partInfo):\n",
    "    max_value, max_index = torch.max(partInfo, dim=0)\n",
    "    return max_value, max_index\n",
    "\n",
    "def changeInfo(partInfo,changeIdx,changeValue,acc=True):\n",
    "    if acc:\n",
    "        partInfo[changeIdx] += changeValue\n",
    "    else:\n",
    "        partInfo[changeIdx] = changeValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3463, 2107, 1766, 1362, 1100, 1068,  964,  904,  813,  755,  751,  713,\n",
       "         702,  637,  629,  629,  624,  604,  599,  598,  579,  565,  546,  542,\n",
       "         540,  532,  531,  521,  515,  510])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortLabelsNUM[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   135837,    334735,    145726,  ...,  79864979, 105840503,\n",
       "          5145424])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = 0\n",
    "partNUM = 8\n",
    "partInfo = torch.zeros(partNUM,dtype=torch.int64)\n",
    "label2part = torch.zeros(torch.max(labelIdx).item()+1,dtype=torch.int64)\n",
    "for index,labelId in enumerate(labelIdx):\n",
    "    value,partid = findMaxPart(partInfo[ids:])    # query current minist\n",
    "    if value.item() < int(1210000/8):\n",
    "        label2part[labelId] = ids\n",
    "        changeInfo(partInfo,ids,sortLabelsNUM[index])    # Modify the current storage table\n",
    "    else:\n",
    "        ids += 1\n",
    "        label2part[labelId] = ids\n",
    "        changeInfo(partInfo,ids,sortLabelsNUM[index])    # Modify the current storage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The small clusters are merged to produce a specific number of clusters\n",
    "partNUM = 8\n",
    "partInfo = torch.zeros(partNUM,dtype=torch.int64)\n",
    "label2part = torch.zeros(torch.max(labelIdx).item()+1,dtype=torch.int64)\n",
    "for index,labelId in enumerate(labelIdx):\n",
    "    value,partid = findMinPart(partInfo)    # query current minist\n",
    "    label2part[labelId] = partid\n",
    "    changeInfo(partInfo,partid,sortLabelsNUM[index])    # Modify the current storage table\n",
    "\n",
    "# At this point, all the small clusters are put into the large cluster, id: 0 - (partNUM-1)\n",
    "trainids\n",
    "trainIdsInPart = label2part[trainLabel.to(torch.int64)] # Finally get the partition in which each training node is located\n",
    "\n",
    "# Finally, nodeInfo[trainids] = trainIdsInPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Has allocated {torch.cuda.memory_allocated() / (1024 ** 3)} GB CUDA Memory\")\n",
    "\n",
    "MAXEDGE = 800000000    # \n",
    "MAXSHUFFLE = 30000000   # \n",
    "\n",
    "def PRgenG(src,dst,nodeInfo,nodeValue):\n",
    "    print(\"test genG...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Has allocated {torch.cuda.memory_allocated() / (1024 ** 3)} GB CUDA Memory\")\n",
    "    template_array = torch.zeros(nodeNUM,dtype=torch.int32)\n",
    "\n",
    "    # Streaming edge data\n",
    "    batch_size = len(src) // MAXEDGE + 1\n",
    "    \n",
    "    src_batches = torch.chunk(src, batch_size, dim=0)\n",
    "    dst_batches = torch.chunk(dst, batch_size, dim=0)\n",
    "    batch = [src_batches, dst_batches]\n",
    "\n",
    "    inNodeTable, outNodeTable = template_array.clone().cuda(), template_array.clone().cuda()\n",
    "    for src_batch,dst_batch in zip(*batch):\n",
    "        src_batch,dst_batch = src_batch.cuda(),dst_batch.cuda()\n",
    "        inNodeTable,outNodeTable = dgl.sumDegree(inNodeTable,outNodeTable,src_batch,dst_batch)\n",
    "    src_batch,dst_batch = None,None\n",
    "    outNodeTable = outNodeTable.cpu() # innodeTable still in GPU for next use\n",
    "\n",
    "    edgeTable = torch.zeros_like(src).to(torch.int32)\n",
    "\n",
    "    tmp_etable = torch.zeros_like(dst_batches[0],dtype=torch.int32).cuda()\n",
    "    for _ in range(3):\n",
    "        offset = 0\n",
    "        acc_nodeValue = torch.zeros_like(nodeValue,dtype=torch.int32)\n",
    "        acc_nodeInfo = torch.zeros_like(nodeInfo,dtype=torch.int32)\n",
    "        for src_batch,dst_batch in zip(*batch):  \n",
    "            batchLen = len(src_batch)\n",
    "            tmp_nodeValue,tmp_nodeInfo = nodeValue.clone().cuda(),nodeInfo.clone().cuda() \n",
    "            src_batch,dst_batch = src_batch.cuda(), dst_batch.cuda()  \n",
    "            tmp_etable.fill_(0)\n",
    "            dgl.per_pagerank(dst_batch,src_batch,tmp_etable,inNodeTable,tmp_nodeValue,tmp_nodeInfo)\n",
    "            edgeTable[offset:offset+batchLen] = tmp_etable[:batchLen].cpu()\n",
    "            tmp_nodeValue, tmp_nodeInfo = tmp_nodeValue.cpu(),tmp_nodeInfo.cpu()\n",
    "            acc_nodeValue += tmp_nodeValue - nodeValue\n",
    "            acc_nodeInfo = acc_nodeInfo | tmp_nodeInfo     \n",
    "            offset += len(src_batch)\n",
    "        nodeValue = nodeValue + acc_nodeValue\n",
    "        nodeInfo = acc_nodeInfo\n",
    "        tmp_nodeValue,tmp_nodeInfo=None,None\n",
    "    src_batch,dst_batch,inNodeTable,tmp_etable = None,None,None,None\n",
    "\n",
    "    return nodeInfo\n",
    "\n",
    "nodeInfo = PRgenG(src, dst, nodeInfo, nodeValue)\n",
    "\n",
    "for i in range(8):\n",
    "    nodeIndex = (nodeInfo & (1 << i)) != 0\n",
    "    nid = torch.nonzero(nodeIndex).reshape(-1).to(torch.int32)\n",
    "    print(f\"{i}: {nid.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl_v09",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
